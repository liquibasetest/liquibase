from awsglue.context import GlueContext
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import rand, lit, round, date_format

# Create a Spark session
spark = SparkSession.builder \
    .appName("ModifyAndWritePartitionedData") \
    .getOrCreate()

# Create GlueContext
glueContext = GlueContext(SparkContext.getOrCreate())

# Read data into a DataFrame
dataframe = spark.read \
    .format("parquet") \
    .option("basePath", "s3://your-bucket/path/to/incomefile") \
    .load("s3://your-bucket/path/to/incomefile")

# Generate random values for string fields
dataframe = dataframe.withColumn("string_field", rand().cast("string"))

# Generate random values for decimal and numeric fields
dataframe = dataframe.withColumn("decimal_field", round(rand() * 100, 2)) \
                     .withColumn("numeric_field", rand() * 100)

# Generate random values for partition_dt and contxt_key_id
dataframe = dataframe.withColumn("partition_dt", date_format(rand() * 10000000000, "yyyyMMdd")) \
                     .withColumn("contxt_key_id", date_format(rand() * 10000000000, "yyMMdd") + rand().cast("int").cast("string"))

# Write data to different partition folders based on partition_dt and contxt_key_id
dataframe.write \
    .partitionBy("partition_dt", "contxt_key_id") \
    .format("parquet") \
    .mode("overwrite") \
    .save("s3://your-bucket/output-path")

# Stop Spark session
spark.stop()
