from awsglue.context import GlueContext
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import rand, lit, sha1

# Create a Spark session
spark = SparkSession.builder \
    .appName("ModifyAndWritePartitionedData") \
    .getOrCreate()

# Create GlueContext
glueContext = GlueContext(SparkContext.getOrCreate())

# Read data into a DataFrame
dataframe = spark.read \
    .format("parquet") \
    .option("basePath", "s3://your-bucket/path/to/incomefile") \
    .load("s3://your-bucket/path/to/incomefile")

# Generate random values for string fields
dataframe = dataframe.withColumn("string_field", sha1(rand()).cast("string"))

# Generate random values for decimal and numeric fields
dataframe = dataframe.withColumn("decimal_field", rand()) \
                     .withColumn("numeric_field", rand() * 100)

# Update partition columns
dataframe = dataframe.withColumn("partition_dt", lit("new_partition_value")) \
                     .withColumn("contxt_key_id", lit("new_contxt_key_value"))

# Write data to different partition folders based on partition_dt and contxt_key_id
dataframe.write \
    .partitionBy("partition_dt", "contxt_key_id") \
    .format("parquet") \
    .mode("overwrite") \
    .save("s3://your-bucket/output-path")

# Stop Spark session
spark.stop()
