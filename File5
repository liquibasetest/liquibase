from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, rand, round, date_format, from_unixtime
from datetime import datetime, timedelta

# Create a Spark session
spark = SparkSession.builder \
    .appName("RepartitionAndRewriteData") \
    .getOrCreate()

# Read data into a DataFrame
dataframe = spark.read \
    .format("parquet") \
    .option("basePath", "s3://your-bucket/path/to/incomefile") \
    .load("s3://your-bucket/path/to/incomefile")

# Define the range for partition_dt
start_date = datetime(2020, 2, 2)
end_date = datetime.now()

# Generate random dates within the specified range
delta = end_date - start_date
random_date = start_date + timedelta(days=rand() * delta.days)

# Convert the random date to the desired format (yyyyMMdd)
new_partition_dt = random_date.strftime("%Y%m%d")

# Repartition the DataFrame based on the contxt_id. This ensures that all records with the same contxt_id will be processed together.
dataframe = dataframe.repartition("contxt_id")

# Update the necessary fields with new values
# For example, change values in "some_field1", "some_field2", "some_field3", "some_field4", and "some_field5" columns
dataframe = dataframe.withColumn("some_field1", lit("new_value1")) \
                     .withColumn("some_field2", lit("new_value2")) \
                     .withColumn("some_field3", lit("new_value3")) \
                     .withColumn("some_field4", lit("new_value4")) \
                     .withColumn("some_field5", lit("new_value5")) \
                     .withColumn("partition_dt", lit(new_partition_dt))

# Write data to new partition folders based on the updated partition_dt and contxt_id
dataframe.write \
    .partitionBy("partition_dt", "contxt_id") \
    .format("parquet") \
    .mode("overwrite") \
    .save("s3://your-bucket/output-path")

# Stop Spark session
spark.stop()
