from awsglue.context import GlueContext
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import rand, lit, round, date_format

# Create a Spark session
spark = SparkSession.builder \
    .appName("ModifyAndWritePartitionedData") \
    .getOrCreate()

# Create GlueContext
glueContext = GlueContext(SparkContext.getOrCreate())

# Read data into a DataFrame
dataframe = spark.read \
    .format("parquet") \
    .option("basePath", "s3://your-bucket/path/to/incomefile") \
    .load("s3://your-bucket/path/to/incomefile")

# Generate random values for string fields
dataframe = dataframe.withColumn("string_field", rand().cast("string"))

# Generate random values for decimal and numeric fields
from pyspark.sql.functions import rand, round, lit

# Generate random integer values in the range from 1,000,000 to 9,999,999
numeric_field = round(rand() * 8999999 + 1000000, 0).cast("int")

# Add the generated numeric field to the DataFrame
dataframe = dataframe.withColumn("numeric_field", numeric_field)


# Generate random values for partition_dt and contxt_key_id
from pyspark.sql.functions import lit, rand, date_format, from_unixtime

# Convert random double to timestamp using from_unixtime()
timestamp_expr = from_unixtime(rand() * 10000000000)

dataframe = dataframe.withColumn("partition_dt", date_format(timestamp_expr, "yyyyMMdd")) \
                     .withColumn("contxt_key_id", date_format(timestamp_expr, "yyMMdd") + lit(rand() * 100000).cast("int").cast("string"))

# Write data to different partition folders based on partition_dt and contxt_key_id
dataframe.write \
    .partitionBy("partition_dt", "contxt_key_id") \
    .format("parquet") \
    .mode("overwrite") \
    .save("s3://your-bucket/output-path")

# Stop Spark session
spark.stop()
