from pyspark.sql import SparkSession

# Create a SparkSession and configure it for Iceberg and Hadoop catalog
spark = SparkSession.builder \
    .appName("IcebergExample") \
    .config("spark.sql.catalog.hadoop_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.hadoop_catalog.type", "hadoop") \
    .config("spark.sql.catalog.hadoop_catalog.warehouse", "s3://your-bucket/warehouse") \
    .config("spark.hadoop.fs.s3a.access.key", "your-access-key") \
    .config("spark.hadoop.fs.s3a.secret.key", "your-secret-key") \
    .config("spark.hadoop.fs.s3a.endpoint", "s3.amazonaws.com") \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .getOrCreate()

# Define the path to the Parquet files
parquet_path = "s3://your-bucket/path/to/parquet/files"

# Read the Parquet files into a DataFrame
df = spark.read.parquet(parquet_path)

# Define the Iceberg table name
iceberg_table = "hadoop_catalog.your_database.your_table"

# Create the table if it doesn't exist
spark.sql(f"""
CREATE TABLE IF NOT EXISTS {iceberg_table} (
    col1 STRING,
    col2 STRING,
    partition_date STRING,
    context_key_id STRING
    -- Add other columns as per your schema
)
USING iceberg
PARTITIONED BY (partition_date, context_key_id)
""")

# Write the DataFrame to the Iceberg table
df.write \
  .format("iceberg") \
  .mode("append") \
  .save(iceberg_table)

# Stop the Spark session
spark.stop()
