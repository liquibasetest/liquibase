from pyspark.context import SparkContext
from awsglue.context import GlueContext
from pyspark.sql import SparkSession
import json

# Initialize SparkContext
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# Read Parquet files from S3
df = spark.read.parquet("s3://path/to/parquet/files/income_file/*/*")

# Get the first record of each file and its schema
first_record_schema_details = {}
for file_path in df.inputFiles():
    first_record_df = spark.read.parquet(file_path).limit(1)
    partition_values = first_record_df.select("partition_date", "contxt_key_id").collect()[0]
    partition_date = partition_values.partition_date
    contxt_key_id = partition_values.contxt_key_id
    schema_json = first_record_df.schema.json()
    file_name = f"schema_details_{partition_date}_{contxt_key_id}.json"
    first_record_schema_details[file_name] = schema_json

# Write schema details to JSON files with names based on partition values
output_folder = "s3://path/to/output/schema_details/"
for file_name, schema_json in first_record_schema_details.items():
    json_output_path = f"{output_folder}{file_name}"
    with open(json_output_path, "w") as f:
        json.dump(json.loads(schema_json), f, indent=4)
