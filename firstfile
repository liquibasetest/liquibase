from pyspark.context import SparkContext
from awsglue.context import GlueContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.utils import AnalysisException

# Initialize SparkContext
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# Define a function to read Parquet files with error handling
def read_parquet_with_error_handling(file_path):
    try:
        return spark.read.parquet(file_path)
    except AnalysisException as e:
        print(f"Error reading file {file_path}: {str(e)}")
        # Write file details to a table or another location for tracking
        error_details = spark.createDataFrame([(file_path, str(e))], ["file_path", "error_message"])
        error_details.write.mode("append").parquet("s3://path/to/output/error_details")
        return None

# Read Parquet files from S3 with partitions and handle errors
df = spark.read.parquet("s3://path/to/parquet/files/income_file/*")
df = df.rdd.flatMap(lambda x: read_parquet_with_error_handling(x)).toDF()

# If there were any files with errors, filter them out from further processing
if df is not None:
    # Get distinct schemas
    schemas = df.schema

    # Collect all distinct column names from all partitions
    all_columns = set()
    for partition_name in df.select("partition_date", "contx_id").distinct().collect():
        partition_df = df.filter((col("partition_date") == partition_name.partition_date) & (col("contx_id") == partition_name.contx_id))
        partition_columns = set(partition_df.columns)
        all_columns.update(partition_columns)

    # Write schema details of each partition to separate files
    for partition_name in df.select("partition_date", "contx_id").distinct().collect():
        partition_df = df.filter((col("partition_date") == partition_name.partition_date) & (col("contx_id") == partition_name.contx_id))
        schema_details = {}
        schema_details["partition_name"] = (partition_name.partition_date, partition_name.contx_id)
        schema_details["schema"] = partition_df.schema.json()

        # Write schema details to file
        partition_schema_df = spark.createDataFrame([schema_details])
        partition_schema_df.write.mode("overwrite").parquet(f"s3://path/to/output/schema_details/partition_{partition_name.partition_date}_{partition_name.contx_id}")

    # Identify schema differences across partitions
    schema_differences = {}
    for partition_name in df.select("partition_date", "contx_id").distinct().collect():
        partition_df = df.filter((col("partition_date") == partition_name.partition_date) & (col("contx_id") == partition_name.contx_id))
        partition_columns = set(partition_df.columns)
        partition_schema_diff = partition_columns.symmetric_difference(all_columns)
        schema_differences[(partition_name.partition_date, partition_name.contx_id)] = list(partition_schema_diff)

    # Write schema differences to another file
    schema_diff_df = spark.createDataFrame(schema_differences.items(), ["partition_name", "schema_difference"])
    schema_diff_df.write.mode("overwrite").parquet("s3://path/to/output/schema_differences")
