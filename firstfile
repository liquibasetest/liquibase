from pyspark.context import SparkContext
from awsglue.context import GlueContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize SparkContext
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# Read Parquet files from S3 with partitions
df = spark.read.parquet("s3://path/to/parquet/files/income_file")

# Get distinct schemas
schemas = df.schema

# Compare schemas
schema_differences = []
for schema in schemas:
    # Assuming you have a reference schema, you can compare against it
    if schema != reference_schema:
        schema_differences.append(schema)

# Write schema differences to another S3 file
schema_diff_df = spark.createDataFrame(schema_differences)
schema_diff_df.write.mode("overwrite").parquet("s3://path/to/output/schema_differences")
