from pyspark.context import SparkContext
from awsglue.context import GlueContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import input_file_name, substring_index

# Initialize SparkContext
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# Read Parquet files from S3
df = spark.read.parquet("s3://path/to/parquet/files/income_file/*/*/")

# Extract partition values from file paths
df = df.withColumn("partition_date", substring_index(substring_index(input_file_name(), "/", -2), "=", -1))
df = df.withColumn("cnt_id", substring_index(substring_index(input_file_name(), "/", -1), "=", -1))

# Get distinct partition values
partitions = df.select("partition_date", "cnt_id").distinct().collect()

# Iterate over each partition and write schema details to separate CSV files
for partition in partitions:
    partition_date = partition.partition_date
    cnt_id = partition.cnt_id
    
    # Filter DataFrame by partition
    partition_df = df.filter((col("partition_date") == partition_date) & (col("cnt_id") == cnt_id))
    
    # Get schema details
    schema_details = [(field.name, field.dataType.simpleString()) for field in partition_df.schema.fields]
    
    # Write schema details to CSV file
    csv_path = f"s3://path/to/output/schema_details/{partition_date}_{cnt_id}_schema.csv"
    with open(csv_path, "w") as f:
        f.write("Field Name,Data Type\n")
        for field_name, data_type in schema_details:
            f.write(f"{field_name},{data_type}\n")
