from pyspark.context import SparkContext
from awsglue.context import GlueContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit
from datetime import datetime, timedelta
import random

# Initialize Spark and Glue contexts
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# Read original Parquet files
original_df = spark.read.parquet("s3://bucket/subfolder1/filename/")

# Define the range of dates
start_date = datetime.strptime("2022-01-01", "%Y-%m-%d")
end_date = datetime.strptime("2022-12-31", "%Y-%m-%d")
date_range = [start_date + timedelta(days=x) for x in range((end_date-start_date).days + 1)]

# Define the number of random partition_dt values needed
num_partitions = 10

# Generate random partition_dt values
random_partition_dt_values = random.sample(date_range, num_partitions)
random_partition_dt_values = [date.strftime("%Y%m%d") for date in random_partition_dt_values]

# Iterate through random partition_dt values
for new_partition_dt in random_partition_dt_values:
    # Create a new DataFrame with modified partition_dt
    modified_df = original_df.withColumn("partition_dt", lit(new_partition_dt))
    
    # Write the modified DataFrame to new Parquet files
    modified_df.write.mode("overwrite").partitionBy("partition_dt", "cont_id").parquet("s3://bucket/subfolder1/filename/")
