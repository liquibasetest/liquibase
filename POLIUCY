I conducted a performance comparison between ABDDW and Parquet files (accessed in Snowflake as external tables) for the TAX and ABC_ACCT_REVIEW tables, each containing 600 fields.
Partitioned Fields: Using partitioned fields (e.g., PARTITION_DATE) in filter conditions allows Snowflake to utilize partition pruning, resulting in query performance that is better or equivalent to ABDDW (Teradata database).
Non-Partitioned Fields: Using non-partitioned fields in Snowflake does not perform as well as in ABDDW because Snowflake needs to scan all partitions in the Parquet file. For example, using EFF_DT in Snowflake is slower as it requires scanning all partitions.
Query Caching: Re-running the same queries without modifications enables Snowflake to use its cache (external or query cache), leading to execution times of just a few seconds.
User Guidance: Business users should be educated to use the PARTITION_DATE format (yyyymmdd) rather than EFF_DT (yyyy-mm-dd) to improve query performance.
Warehouse Sizing: A smaller warehouse size is sufficient for selecting records based on partition dates. For complex operations, a medium warehouse size performs better.
Query Planning Insights: Most of the query execution time is spent scanning partitions (fetching data from S3 takes 75% to 85% of the time), with the remaining time used for actual SQL query processing.
Our Setup:
Data Storage: Files are stored in S3 in Parquet format with Snappy compression.
External Table Access: These Parquet files are accessed as external tables in Snowflake.
View Creation: A view is created on top of the external table to select the latest context ID.
TAX Table: Compared ABDDW (production environment) vs. Snowflake (production environment) with 5 GB of data.
ABC_ACCT_REVIEW Table: Compared ABDDW (production environment) vs. Snowflake (dev environment) with a simulated dataset in Snowflake (60% of production data, totaling 1.2 TB with 1000 partitions).
Additional Table: Created the DWH.ACCT table for potential future sharing by ABC or linking by SAS with ABC Risk tables.

Performance Comparison Findings:
When using partitioned fields in filter conditions, Snowflake leverages partition pruning, resulting in query performance that matches or exceeds ABDDW (Teradata database).
However, using non-partitioned fields in Snowflake does not perform as well as in ABDDW, as Snowflake needs to scan all partitions in the Parquet file. When i had interaction with TALX business user, mostly they use the eff_dt field (
Query caching allows Snowflake to use its cache for re-run queries without modifications, leading to execution time faster
Business users are advised to use the PARTITION_DATE format rather than EFF_DT to improve query performance.
Warehouse sizing varies; a smaller warehouse size suffices for selecting records based on partition dates, while a medium warehouse size performs better for complex operations.
Query planning insights reveal that most query execution time is spent scanning partitions, with fetching data from S3 taking 75% to 85% of the time.

In the ABCDW application, index fields are established, typically based on frequently filtered fields as per conventional database practices. For instance, the "acct_ab" field is indexed in ICDW, but such indexing isn't present in ABCDW. Consequently, queries in ABCDW often exhibit superior performance.

Please find the deck which shows each and every criteria of execution for both the TAX and ABC_ACCT_REVIEW details.

In some cases, for larger files, business users might only use non-partitioned fields in the filter condition. When this happens, query performance can be suboptimal if the underlying file is in Parquet format.So i performed the POC on unmanaged Iceberg table in snowflake.  To perform the POC, 1.2 TB of ABC_ACCT_REVIEW Parquet file into Iceberg format and stored it in S3. I used the Hadoop catalog to maintain the table, as the Glue catalog was not included in the performance comparison due to the associated costs for CRUD operations. An unmanaged Iceberg table was created in Snowflake using the object integration method, pointing to the Iceberg table metadata managed by Hadoop.

Initial performance comparisons indicate that Iceberg provides better performance for filtering based on both partitioned and non-partitioned fields compared to the Parquet format. I will provide more insights once the comparison is complete.


ICEBERG FORMAT


Below is a detailed comparison between accessing Parquet files in Snowflake as external tables and accessing an Iceberg table via object integration (unmanaged Iceberg table):

Storage Efficiency: Converting from Parquet to Iceberg format results in a noticeable reduction in stored size in S3, with a decrease of approximately 30%.

Flexibility in Data Access: Even after conversion to Iceberg format, the data remains accessible as a Parquet file. This allows for both metadata access as an Iceberg table and direct data access as a Parquet file.

Performance in Snowflake Environment:

Partitioned Field Access: Both Parquet and Iceberg formats perform similarly when accessing partitioned fields for simple field selection.

Aggregation Performance: Iceberg format demonstrates better performance compared to Parquet files when performing aggregation operations.

Querying Non-Partitioned Fields: Iceberg format significantly outperforms Parquet files when querying non-partitioned fields.

Overall Performance: In all aspects, the unmanaged Iceberg table provides superior performance compared to accessing S3 Parquet files.

Additionally, while Iceberg provides features such as ACID (Atomicity, Consistency, Isolation, Durability) properties for ensuring data integrity and reliability, these capabilities were not tested as they are not relevant to our current use case. Similarly, version control, although available, was not tested as it does not apply to our scenario.
