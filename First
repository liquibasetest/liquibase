from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, LongType

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Create Iceberg Table") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.access.key", "YOUR_ACCESS_KEY") \
    .config("spark.hadoop.fs.s3a.secret.key", "YOUR_SECRET_KEY") \
    .getOrCreate()

# Initialize SparkContext
sc = spark.sparkContext

# Configure Hadoop Catalog with S3
sc._jsc.hadoopConfiguration().set("spark.hadoop.iceberg.catalog", "hadoop.catalog.s3")
sc._jsc.hadoopConfiguration().set("spark.hadoop.iceberg.catalog.s3.warehouse", "s3://your-bucket-name/catalog/warehouse")

# Define schema
schema = StructType([
    StructField("id", LongType(), True),
    StructField("name", StringType(), True)
])

# Create Iceberg table
spark.sql("CREATE TABLE IF NOT EXISTS your_database.your_table_name USING iceberg OPTIONS ('catalog'='hadoop.catalog.s3', 'warehouse'='s3://your-bucket-name/catalog/warehouse') AS SELECT * FROM VALUES (1, 'John'), (2, 'Jane') AS t(id, name)")
