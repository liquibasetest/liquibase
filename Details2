from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("ParquetToIceberg") \
    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark3-runtime:0.14.1") \
    .getOrCreate()

# Read Parquet files from S3
df = spark.read.parquet("s3a://your-s3-bucket/path/to/parquet")

# Define the partition columns
partition_columns = ['part_yr', 'partition_date']

# Write data to Iceberg table on S3 using SQL
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS iceberg_table
    USING iceberg
    PARTITIONED BY ({', '.join(partition_columns)})
    LOCATION 's3a://your-iceberg-bucket/path/to/iceberg_table'
    AS
    SELECT *
    FROM parquet_data
""")

# Stop Spark session
spark.stop()

from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder \
    .appName("ParquetToIceberg") \
    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark3-runtime:0.14.1") \
    .config("spark.sql.catalog.iceberg_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
    .config("spark.sql.catalog.spark_catalog.type", "hive") \
    .config("spark.sql.catalog.spark_catalog.uri", "thrift://<hive-metastore-host>:9083") \
    .getOrCreate()

# Read Parquet files from S3
df = spark.read.parquet("s3a://your-s3-bucket/path/to/parquet")

# Register dataframe as a temporary view
df.createOrReplaceTempView("parquet_data")

# Define the partition columns
partition_columns = ['part_yr', 'partition_date']

# Write data to Iceberg table on S3 using SQL
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS iceberg_table
    USING iceberg
    PARTITIONED BY ({', '.join(partition_columns)})
    LOCATION 's3a://your-iceberg-bucket/path/to/iceberg_table'
    AS
    SELECT *
    FROM parquet_data
""")

# Stop Spark session
spark.stop()

